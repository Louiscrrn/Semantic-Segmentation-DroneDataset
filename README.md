# Drone Semantic Segmentation

> Deep Learning project for binary and multiclass semantic segmentation on drone imagery.  
> Developed as part of the SICOM S9 "Acceleration Material" course.

## ğŸ“Œ Overview

This repository provides a complete pipeline for semantic segmentation on drone-acquired datasets using state-of-the-art models like **UNet**, **SegFormer**, and **UFormer**.  
It supports training, evaluation, prediction visualization, and distributed training (for Gricad cluster).

---

## ğŸ“ Project Structure

```
semantic-segmentation-drone-data/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ python/
â”‚   â”‚   â”œâ”€â”€ droneDataset.py     # Dataset and preprocessing logic
â”‚   â”‚   â”œâ”€â”€ metrics.py          # Metrics: PA, MPA, IoU, mIoU
â”‚   â”‚   â”œâ”€â”€ model.py            # Model definitions (UNet, SegFormer, UFormer)
â”‚   â”‚   â”œâ”€â”€ trainer.py          # Training, validation, and testing logic
â”‚   â”‚   â”œâ”€â”€ vizualization.py    # Visualization utilities
â”‚   â”œâ”€â”€ get_curves.py           # Plot training curves from CSV logs
â”‚   â”œâ”€â”€ main.py                 # Train/validate/test a model
â”‚   â””â”€â”€ predict.py              # Generate predictions from a trained model
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ MultiUnet/
â”‚   â”‚   â””â”€â”€ predictions.zip
â”‚   â””â”€â”€ SegFormer/
â”‚       â””â”€â”€ predictions.zip
â”œâ”€â”€ config.yaml                 # Main configuration file
â””â”€â”€ README.md                   # Project documentation
```

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/semantic-segmentation-drone-data.git
cd semantic-segmentation-drone-data
```

### 2. Install Dependencies

Make sure youâ€™re using **Python â‰¥3.8** and a virtual environment:

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
pip install -r requirements.txt
```

---

## âš™ï¸ Configuration

Update the `config.yaml` file to modify:

- Dataset paths
- Model architecture (`UNet`, `SegFormer`, etc.)
- Training hyperparameters
- Output paths
- Distributed training settings

---

## ğŸ“ˆ Usage

### ğŸ‹ï¸ Train a model

```bash
python ./src/main.py
```

> Make sure `distributed::active` is disabled in `config.yaml` if running locally.

### ğŸ” Make predictions

```bash
python ./src/predict.py
```

> Update the model checkpoint path in `config.yaml`.

### ğŸ“Š Plot learning curves

```bash
python ./src/get_curves.py
```

---

## ğŸ–¥ï¸ Distributed Training on Gricad

### ğŸ”§ Setup

1. Enable distributed training:
   ```yaml
   distributed:
     active: 1
   ```

2. Create a `.h` script with the following:
   ```bash
   export CUDA_VISIBLE_DEVICES=0,1,2,3
   torchrun --nproc_per_node=4 src/main.py
   ```

> Use `localhost` as master node (Gricad allocates it). If port conflicts occur, change it manually.

---

## ğŸ“¦ Requirements

All dependencies are listed in `requirements.txt`.

Key packages include:

- PyTorch
- torchvision
- segmentation_models_pytorch
- transformers
- matplotlib / plotly
- PyYAML / pandas / PIL

---

## ğŸ“Œ Notes

- Predictions and curves **must be run with distributed mode off**.
- Some models require **custom install steps** depending on your PyTorch version (e.g. for HuggingFace SegFormer).
- Set `TF_ENABLE_ONEDNN_OPTS=0` for compatibility when using CPU backends.

---

## ğŸ“· Results

Below is a sample prediction result from the **MultiUNet** model on the drone dataset:

<p align="center">
  <img src="outputs/MultiUnet/predictions/predictions/MultiUnet_pred_40.png" alt="MultiUNet Prediction" width="1000"/>
</p>

> The UNet model successfully segments large and clearly defined classes such as moving objects and landable areas. However, it shows limitations with smaller or less contrasted elements, sometimes misclassifying obstacles or blending class boundaries. This result highlights UNet's solid performance in general structure recognition, but also its relative weakness in fine-grained or context-dependent segmentation tasks.

A sample prediction result from the **SegFormer** model

<p align="center">
  <img src="outputs/SegFormer/predictions/predictions/SegFormer_pred_12.png" alt="SegFormer Prediction" width="1000"/>
</p>

> This SegFormer prediction shows robust performance on large and structured regions, especially the **water body**, which is segmented with high precision. The model also correctly identifies many surrounding **obstacles** and patches of **nature**, even in complex, cluttered urban scenery. While some minor confusion persists between **moving** and **obstacle** classes in dense zones, the overall segmentation is consistent and well-aligned with the ground truth. This reflects SegFormer's strong ability to model long-range dependencies and handle heterogeneous scenes with fine details.

---

## ğŸ§‘â€ğŸ’» Contributors

- Louis Carron â€” [Main Developer]

---